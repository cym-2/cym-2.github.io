---
title: ai paper review 3
author: CYM
date: 2021-01-31 22:00:00
categories: [ai]
tags: [ai_practice]
---

# 모델 성능 측정 지표
<br>
<br>

![img](https://github.com/cym-2/cym-2.github.io/blob/main/assets/img/posts/apr1/001.png?raw=true)

- True Positive: 모델은 A일 것이라 예측하였고 실제로 그런 경우<br>
- True Negative: 모델은 A가 아닐 것이라 예측하였고 실제로 그런 경우<br>
- False Positive: 모델은 A일 것이라 예측하였고 실제로는 아닌 경우<br>
- False Negative: 모델은 A가 아닐 것이라 예측하였고 실제로는 아닌 경우<br>
<br>
<br>

## Accuracy
<br>

**올바르게 예측된 데이터의 수를 전체 데이터의 수로 나눈 값.**<br>

![img](https://github.com/cym-2/cym-2.github.io/blob/main/assets/img/posts/apr1/002.PNG?raw=true)

<br>
<br>

## Precision
<br>

**정밀도: 모델이 True로 예측한 데이터 중 실제로 True인 데이터의 수**

![img](https://github.com/cym-2/cym-2.github.io/blob/main/assets/img/posts/apr1/004.PNG?raw=true)

```
precision = 1.0 * num_same / len(prediction_Char)
```

<br>
<br>

## Recall
<br>

**재현율: 실제로 True인 데이터를 모델이 True라고 인식한 데이터의 수**

![img](https://github.com/cym-2/cym-2.github.io/blob/main/assets/img/posts/apr1/003.PNG?raw=true)

```
recall = 1.0 * num_same / len(ground_truth_Char)
```

*Recall과 Precision은 trade-off되는 관계*
<br>
<br>

## F1 Score
<br>

**Precision과 Recall의 조화평균**<br>

![img](https://github.com/cym-2/cym-2.github.io/blob/main/assets/img/posts/apr1/005.PNG?raw=true)

```
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

labels = [1, 0, 0, 1, 1, 1, 0, 1, 1, 1]	# 실제 labels
guesses = [0, 1, 1, 1, 1, 0, 1, 0, 1, 0]	# 에측된 결과

print(accuracy_score(labels, guesses))	# 0.3
print(precision_score(labels, guesses))	# 0.5
print(recall_score(labels, guesses))	# 0.42
print(f1_score(labels, guesses))	# 0.46
```

