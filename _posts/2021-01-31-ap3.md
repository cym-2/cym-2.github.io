---
title: ai paper review 3
author: CYM
date: 2021-01-31 22:00:00
categories: [ai]
tags: [ai_paper]
---

![img](https://github.com/cym-2/cym-2.github.io/blob/main/assets/img/posts/ap3/001.PNG?raw=true)

<br>
<br>

![img](https://github.com/cym-2/cym-2.github.io/blob/main/assets/img/posts/ap3/002.png?raw=true)

<br>

**ELECTRA**<br>
Replaced Token Detection, 즉 generator에서 나온 token을 보고<br>
discriminator에서 "real" token인지 "fake" token인지 판별하는 방법으로 학습.<br> 
이 방법은 모든 input token에 대해 학습할 수 있다는 장점을 가지며, BERT 등과 비교했을 때 더 좋은 성능을 보임.<br>
<br>
<br>
<br>

---

<br>

**BERT**<br>
BERT와 GPT모델은 모두 트랜스포머 블록을 사용하고 있음.<br>
그렇다면 GPT는 왜 단어들을 양방향으로 보지 못하는 것일까?<br>
그 이유는 GPT가 언어 모델이기 때문.<br>
GPT는 주어진 단어 시퀀스를 가지고 그 다음 단어를 예측하는 과정에서 학습.<br>
이 경우 현재 입력 단어 이후의 단어를 모델에게 알려주는 것은 반칙.<br>
언어 모델은 주어진 시퀀스를 가지고 다음 단어를 맞춰야 하는데, 맞춰야 할 정답을 미리 알려줄 수는 없기 때문.<br>
이 문제를 극복하기 위해 마스크 언어 모델 제안.<br>
주어진 시퀀스 다음 단어를 맞추는 것에서 벗어나, 일단 문장 전체를 모델에 알려주고,<br>
빈칸(MASK)에 해당하는 단어가 어떤 단어일 지 예측하는 과정을 학습해보자는 아이디어.<br>
BERT의 프리트레인 태스크에는 크게 마스크 언어 모델, 다음 문장인지 여부 맞추기(Next Sentence Prediction, NSP) 두 가지가 있음.<br>
BERT 모델은 트랜스포머 인코더를 일부 변형한 아키텍처.<br>
ReLU대신 GELU사용.<br>
<br>
<br>
<br>

---

<br>

**단어 임베딩**<br>
비정형화된 Text를 숫자로 바꿔줌으로써 사람의 언어를 컴퓨터의 언어로 번역하는 것<br>
<br>
<br>
<br>

---

<br>

**Word2Vec**<br>
CBOW는 주변에 있는 문맥 단어들을 가지고 타깃 단어 하나를 맞추는 과정에서 학습.<br>
Skip-gram은 타깃 단어를 가지고 주변 문맥 단어가 무엇일지 예측하는 과정에서 학습.<br>
Skip-gram이 같은 말뭉치로도 더 많은 학습 데이터를 확보할 수 있어 임베딩 품질이 CBOW보다 좋은 경향 있음.<br>
positive sample이란 타깃 단어와 그 주변에 실제로 등장한 문맥 단어 쌍.<br>
negative sample이란 타깃 단어와 그 주변에 등장하지 않은 단어(말뭉치 전체에서 랜덤 추출) 쌍.<br>
네거티프 샘플링은 타깃 단어와 문맥 단어 쌍이 주어졌을 때<br> 
해당 쌍이 posive sample인지 negative sample인지 이진 분류 하는 과정에서 학습.<br>
네거티브 샘플링 방식으로 학습하게 되면 <br>
1개의 positive sample과 k개의 negaive sample만 계산하면 되기 때문에 기존 방법보다 계산량 적음<br> 
<br>
<br>
<br>

---

<br>

**FastText**<br>
각 단어를 문자character 단위n-gram으로 표현.<br>

```
# '시나브로'의 n=3인 문자 단위 n-gram
<시나, 시나브, 나브로, 브로>, <시나브로>
```

FastText 모델에서는 시나브로라는 단어의 임베딩을 5개의 문자 단위 n-gram 벡터의 합으로 표시.<br>
