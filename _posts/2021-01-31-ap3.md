---
title: ai paper review 3
author: CYM
date: 2021-01-31 22:00:00
categories: [ai]
tags: [ai_paper]
---

![img](https://github.com/cym-2/cym-2.github.io/blob/main/assets/img/posts/ap3/001.PNG?raw=true)

<br>
<br>

![img](https://github.com/cym-2/cym-2.github.io/blob/main/assets/img/posts/ap3/002.png?raw=true)

<br>
ELECTRA는 Replaced Token Detection, 즉 generator에서 나온 token을 보고<br>
discriminator에서 "real" token인지 "fake" token인지 판별하는 방법으로 학습.<br> 
이 방법은 모든 input token에 대해 학습할 수 있다는 장점을 가지며, BERT 등과 비교했을 때 더 좋은 성능을 보임.<br>
<br>
<br>

**BERT**
<br>
BERT와 GPT모델은 모두 트랜스포머 블록을 사용하고 있음.<br>
그렇다면 GPT는 왜 단어들을 양방향으로 보지 못하는 것일까?<br>
그 이유는 GPT가 언어 모델이기 때문.<br>
GPT는 주어진 단어 시퀀스를 가지고 그 다음 단어를 예측하는 과정에서 학습.<br>
이 경우 현재 입력 단어 이후의 단어를 모델에게 알려주는 것은 반칙.<br>
언어 모델은 주어진 시퀀스를 가지고 다음 단어를 맞춰야 하는데, 맞춰야 할 정답을 미리 알려줄 수는 없기 때문.<br>
이 문제를 극복하기 위해 마스크 언어 모델 제안.<br>
주어진 시퀀스 다음 단어를 맞추는 것에서 벗어나, 일단 문장 전체를 모델에 알려주고,<br>
빈칸(MASK)에 해당하는 단어가 어떤 단어일 지 예측하는 과정을 학습해보자는 아이디어.<br>
BERT의 프리트레인 태스크에는 크게 마스크 언어 모델, 다음 문장인지 여부 맞추기(Next Sentence Prediction, NSP) 두 가지가 있음.<br>
BERT 모델은 트랜스포머 인코더를 일부 변형한 아키텍처.<br>
ReLU대신 GELU사용.<br>
<br>
<br>
