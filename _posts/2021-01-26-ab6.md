---
title: ai basic 6
author: CYM
date: 2021-01-26 22:00:00
categories: [ai]
tags: [ai_basic]
---

# Deep Learning
<br>
<br>

**0131.** 딥러닝: 층을 깊게 한 신경망.<br>
<br>

**0132.** 데이터 확장data augmentation: 정확도 개선에 아주 효과적.<br>

- 입력 이미지(훈련 이미지)를 알고리즘을 동원해 '인위적'으로 확장.<br>
- 입력 이미지에 미세한 변화를 주어 이미지의 개수를 늘림.<br>
- 이미지 일부를 잘라내는 crop, 좌우를 뒤집는 flip, 밝기 등의 외형 변화, 확대/축소 등의 스케일 변화<br>

**0133.** '층을 깊게 하는 것'의 중요성.<br>

- 신경망의 매개변수 수가 줄어들음. 층을 깊게 한 신경망은 깊지 않은 경우보다 적은 매개변수로 같은 혹은 그 이상 수준의 표현력을 달성할 수 있음.<br>

![img](https://media.vlpt.us/post-images/dscwinterstudy/d4af9990-4192-11ea-bf7e-239e36e1bc11/5by5%ED%95%A9%EC%84%B1%EA%B3%B1.png)

![img](https://media.vlpt.us/post-images/dscwinterstudy/d9594b80-4192-11ea-b7ab-b932b78555b8/3by3%ED%95%A9%EC%84%B1%EA%B3%B1.png)

  - 5x5의 합성곱 연산 1회는 3x3의 합성곱 연산을 2회 수행하여 대체할 수 있음.<br>
  - 전자의 매개변수 수가 25개(5x5)인 반면, 후자는 총 18개(2x3x3)이며, 매개변수는 층을 반복할수록 적어짐.<br> 
  - 그 개수의 차이는 층이 깊어질 수록 커짐. 3x3의 합성곱 연산을 3회 반복하면 매개변수는 27개지만, 같은 크기의 영역을 1회의 합성곱 연산으로 '보기'위해서는 7x7크기의 필터, 즉 매개변수 49개 필요.<br>
  - 작은 필터를 겹쳐 신경망을 깊게 할 때의 장점은 매개변수 수를 줄여 넓은 수용 영역receptive field(뉴런에 변화를 일으키는 국소적인 공간 영역)를 소화할 수 있다는 것.
  게다가 층을 거듭하면서 ReLU 등의 활성화 함수를 합성곱 계층 사이에 끼움으로써 신경망 표현력이 개선됨.
  이는 활성화 함수가 신경망에 '비선형'힘을 가하고, 비선형 함수가 겹치면서 더 복잡한 것도 표현할 수 있게 되기 때문.<br>

- 학습의 효율성: 층을 깊게 함으로써 학습 데이터의 양을 줄여 학습을 고속으로 수행할 수 있음.<br>
  - 합성곱 계층에서는 에지 등의 단순한 패턴에 뉴런이 반응하고 층이 깊어지면서 텍스처와 사물의 일부와 같이 점차 더 복잡한 것에 반응.<br>
  - '개'를 인식할 때, 얕은 신경망에서 해결하려면 합성곱 계층은 개의 특징 대부분을 한 번에 '이해'해야 함. 
  개의 특징 이해하려면 변화가 풍부하고 많은 학습 데이터 필요, 결과적으로 학습시간 오래 걸림<br>
  - 신경망을 깊게 하면 학습해야 할 문제를 계층적으로 분해할 수 있음. 각 층이 학습해야 할 문제를 더 단순한 문제로 대체. <br>
  
- 정보를 계층적으로 전달할 수 있음.<br>
  - 예를 들어 에지를 추출한 층의 다음 층은 에지 정보를 쓸 수 있고, 더 고도의 패턴을 효과적으로 학습하리라 기대할 수 있음.<br>
  - 층을 깊이 함으로써 각 층이 학습해야 할 문제를 '풀기 쉬운 단순한 문제'로 분해할 수 있어 효율적으로 학습할 수 있음.<br>
  
<br>

**0134.** VGG

![img](https://media.vlpt.us/post-images/dscwinterstudy/ea967bb0-4193-11ea-82e0-f10b8886fb3a/VGG.png)

- 합성곱 계층과 풀링 계층으로 구성되는 기본적인 CNN<br>
- 비중있는 층(합성곱 계층, 완전연결 계층)을 모두 16층(혹은 19층)으로 심화함.<br>
- 3x3의 작은 필터를 사용한 합성곱 계층을 연속으로 거침.
- 합성곱 계층을 2~4회 연속으로 풀링 계층을 두어 크기를 절반으로 줄이는 처리를 반복.<br>
- 마지막에는 완전연결 계층을 통과시켜 결과를 출력.<br>
- 구성이 간단하여 응용하기 좋음.<br>

<br>

**0135.** GoogLeNet

![img](https://miro.medium.com/max/2160/1*ZFPOSAted10TPd3hBQU8iQ.png)

- GoogLeNet은 세로 방향 깊이뿐 아니라 가로 방향도 깊음.<br>
- GoogLeNet에는 가로 방향에 '폭'이 있음. 이를 인셉션 구조라 함.<br>

![img](https://media.vlpt.us/post-images/dscwinterstudy/8556b550-419b-11ea-a3b8-fd5b14e3378a/%EA%B5%AC%EA%B8%80%EB%84%B7%EC%9D%B8%EC%85%89%EC%85%98.png)

- 인셉션 구조는 크기가 다른 필터(와 풀링)를 여러 개 적용하여 그 결과를 결합.<br>
- 1x1의 합성곱 연산은 채널 쪽으로 크기를 줄이는 것으로, 매개변수 제거와 고속 처리에 기여.<br>

<br>

**0136.** ResNet: 딥러닝 학습에서 층이 지나치게 깊으면 학습이 잘 되지 않고, 오히려 성능이 떨어지는 경우 많음. 그런 문제를 해결 하기 위해 스킵 연결skip connection을 도입.<br>

![img](https://media.vlpt.us/post-images/dscwinterstudy/fb4e1da0-4193-11ea-9037-c1df5ac05aa2/ResNet.png)

- 스킵 연결: 입력 데이터를 합성곱 계층을 건너 뛰어 출력에 바로 더하는 구조.<br>
- 단축 경로가 없었다면 두 합성곱 계층의 출력이 F(x)가 되나, 스킵 연결로 인해 F(x) + x가 됨.<br>
- 스킵 연결은 층이 깊어져도 학습을 효율적으로 할 수 있도록 해주는데, 이는 역전파 때 스킵 연결이 신호 감쇠를 막아주기 때문.<br>
- 스킵 연결은 입력 데이터를 '그대로' 흘리는 것으로, 역전파 때도 상류의 기울기를 그대로 하류에 보냄. 핵심은 상류의 기울기에 아무런 수정도 가하지 않고 '그대로'흘린 다는 것.
그래서 스킵 연결로 기울기가 작아지거나 지나치게 커질 걱정 없이 앞 층에 '의미 있는 기울기'가 전해지리라 기대할 수 있음. 
층을 깊게 할수록 기울기가 작아지는 소실 문제를 스킵 연결이 줄여줌.<br>
- ResNet은 VGG신경망을 기반으로 스킵 연결을 도입하여 층을 깊게 함.<br>

<br>

**0137.** 전이 학습transfer learning: 학습된 가중치(혹은 그 일부)를 다른 신경망에 복사한 다음, 그 상태로 재학습을 수행.<br>

- 예를 들어 VGG와 구성이 같은 신경망을 준비하고, 미리 학습된 가중치를 초깃값으로 설정한 후, 새로운 데이터셋을 대상으로 재학습(fine tuning) 수행.<br>
- 전이 학습은 보유한 데이터셋이 적을 때 특히 유용한 방법.<br>

<br>

**0138.** CPU는 연속적인 복잡한 계산을, GPU는 대량 병렬 연산을 잘 처리.<br>
<br>

**0139.** 신경망은 입력 이미지에 노이즈가 조금 섞여 있어도 출력 결과가 잘 달라지지 않는 강건함을 보여줌. 
이런 견고성 덕분에 신경망을 흐르는 데이터를 '퇴화'시켜도 출력에 주는 영향은 적음.
지금까지의 실험으로는 딥러닝은 16비트 반밀정도half-precision만 사용해도 학습에 문제가 없음.
딥러닝을 고속화하기 위해 비트를 줄이는 기술은 앞으로 주시해야 할 분야. 특히 딥러닝을 임베디드용으로 이용할 때.<br>
<br>

**0140.** 사물 검출: 이미지 속에 담긴 사물의 위치와 종류(클래스)를 알아내는 기술.

![img](https://media.vlpt.us/post-images/dscwinterstudy/0b0c6b70-4194-11ea-9037-c1df5ac05aa2/rcnn.png)

- R-CNN(Regions with Convolutional Neural Network)<br>
- 이미지를 사각형으로 변형하거나 분류할 때 서포트 벡터 머신SVM을 사용.<br>
- Selective Search 기법 사용.<br>
- 최근에는 이 후보 영역 추출까지 CNN으로 처리하는 Faster R-CNN 기법 등장.<br>

<br>

**0141.** 분할segmentation: 이미지를 픽셀 수준으로 분류.<br>

![img](https://media.vlpt.us/post-images/dscwinterstudy/4082f170-4194-11ea-82e0-f10b8886fb3a/fcn.png)

- FCN(Fully Convolutional Network): 합성곱 계층으로만 구성된 네트워크. 완전연결 계층을 '같은 기능을 하는 합성곱 계층'으로 바꿈.<br>
- FCN의 마지막에 수행하는 확대는 이중 선형 보간bilinear interpolation에 의한 선형 확대. 역합성곱deconvolution 연산으로 구현.<br>

<br>

**0142.** 사진 캡션 생성.

![img](https://github.com/cym-2/cym-2.github.io/blob/main/assets/img/posts/ab6/142.png)

- NIC(Neural Image Caption): 심층 CNN과 자연어를 다루는 순환 신경망Reccurent Neural Network으로 구성됨.<br>
- NIC는 CNN으로 사전에 특징을 추출하고, 그 특징을 RNN에 넘김. RNN은 CNN이 추출한 특징을 초깃값으로 해서 텍스트를 '순환적'으로 생성.<br>
- 멀티모델 처리mutimodal processing: 사진이나 자연어와 같은 여러 종류의 정보를 조합하고 처리하는 것.<br>

<br>

**0143.** 이미지 스타일(화풍) 변환: A Neural Algorithm of Artistic Style, 네트워크의 중간 데이터가 콘텐츠 이미지의 중간 데이터와 비슷해지도록 학습.<br>
<br>

**0144.** 이미지 생성.<br>

- DCGAN(Deep Convolutional Generative Adversarial Network): 이미지를 생성하는 과정을 모델화.<br>
- 생성자Generator와 식별자Discriminator로 불리는 2개의 신경망을 이용.<br>
- 생성자가 진짜와 똑같은 이미지를 생성하고 식별자는 그것이 진짜인지 판정.<br>
- 이렇게 둘의 능력을 부지런히 갈고닦게 한다는 개념이 GAN 기술의 재미있는 점.<br>

<br>

**0145.** 강화학습(Deep Q-Network)

![img](https://media.vlpt.us/post-images/dscwinterstudy/7d3015d0-4194-11ea-bf7e-239e36e1bc11/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5%EA%B8%B0%EB%B3%B8%ED%8B%80.png)

- '가르침'에 의존하는 '지도학습'과는 다른, 사람이 시행착오를 겪으며 배우듯 컴퓨터도 시행착오 과정에서 스스로 학습하게 하려는 '강화학습'reinforcement learning<br>
- 강화학습에서는 에이전트라는 것이 환겨에 맞게 행동을 선택하고, 그 행동에 의해서 환경이 변한다는 게 기본적인 틀.<br>
- 환경이 변화하면 에이전트는 어떠한 보상을 얻음.<br>
- 강화학습의 목적은 더 나은 보상을 받는 쪽으로 에이전트의 행동 지침을 바로잡는 것. 에이전트는 더 좋은 보상을 받기 위해 스스로 학습.<br>

<br>
