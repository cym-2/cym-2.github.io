---
title: ai paper review 1
author: CYM
date: 2021-01-27 22:00:00
categories: [ai]
tags: [ai_paper]
---

![](https://github.com/cym-2/cym-2.github.io/blob/main/assets/img/posts/ap1/01.PNG?raw=true)

<br>
<br>

- 매우 간단한 구조의 CNN을 활용해서 문장 분류에서 상당한 효율.<br>
- 이 논문에서는 한개의 layer를 사용하는 CNN에 대해서 소개.<br> 
- 그리고 CNN에 사용할 단어 벡터는 1,000억개의 단어를 사전학습한 벡터.<br> 
- 여기서는 이 벡터를 수정하거나 하지 않고 Network의 parameter만 학습.<br> 
<br>
<br>

## Model

![img](https://github.com/cym-2/cym-2.github.io/blob/main/assets/img/posts/ap1/02.png?raw=true)

**01.** input은 k-dimension의 단어 벡터. 문장의 i번 째 단어는 다음과 같음.

![img](https://github.com/cym-2/cym-2.github.io/blob/main/assets/img/posts/ap1/03.PNG?raw=true)

**02.** 문장은 n개의 단어를 concat하여 사용하는데, 필요하다면 padding을 추가. 따라서 다음과 같이 표현됨.

![img](https://github.com/cym-2/cym-2.github.io/blob/main/assets/img/posts/ap1/04.PNG?raw=true)

**03.** ⊕는 concatenation 연산자. 위의 sentence에 convolution. 필터는 hk크기의 벡터의 모양. h개의 단어에 적용되서 새로운 feature를 뽑아냄.

![img](https://github.com/cym-2/cym-2.github.io/blob/main/assets/img/posts/ap1/05.PNG?raw=true)

**04.** convolution 연산을 하면 feature ci가 만들어진다.

![img](https://github.com/cym-2/cym-2.github.io/blob/main/assets/img/posts/ap1/06.PNG?raw=true)

**05.** b∈R는 bias이고 f는 tanh와 같은 non-linear 함수. 이 필터는 sliding하면서 sentence [x1:h,x2:h+1,...,xn−h+1:n]에 대해서 각각 적용되서 feature map c를 만들음.

![img](https://github.com/cym-2/cym-2.github.io/blob/main/assets/img/posts/ap1/07.PNG?raw=true)

**06.** 만들어진 feature map에 max-over-time pooling(Collobert et al., 2011)을 하고 최대 값을 뽑아냄.

![img](https://github.com/cym-2/cym-2.github.io/blob/main/assets/img/posts/ap1/08.PNG?raw=true)

**07.** 위 값은 특정 필터에 상응하는 feature가 됨. feature map에서 가장 중요한 값(높은 값)을 뽑아내는 방법. 
이러한 방법을 사용하면 하나의 필터에 대해서 하나의 feature를 뽑아 냄.<br>
<br>

**08.** 이러한 피쳐들은 penultimate layer를 구성하고 fully connected sofmax layer로 통과시켜서 라벨에 대한 확률 분포를 만들어 냄.<br>
<br>

**09.** 그림을 보면 input에서 2개의 channel이 존재한다. 하나는 위에서 설명한 static한 word vector들을 모아둔 것이고, 
나머지는 backpropagation을 통해 fine tuning 한 것. 
즉 두 개의 channel에 대해서 filter를 적용시킨 후 더해서 사용. 
1개의 channel만 사용한다면 더하는 과정 없이 바로 사용하면 됨.
<br>
<br>

## Regularization
<br>

**10.** 정규화를 위해 dropout과 l2-norm 정규화를 사용.<br>
<br>

**11.** dropout을 penultimate layer에서 마지막 layer로 가는 파라미터에서 사용했는데 이를 사용함으로써 co-adaptation(어떤 뉴런이 다른 특정 뉴런에 의존적으로 변하는 것) 방지.<br>
<br>

**12.** penultimate layer를 z=[c^1,…,c^m]이라 하자. 그러면,

![img](https://github.com/cym-2/cym-2.github.io/blob/main/assets/img/posts/ap1/09.PNG?raw=true)

여기서 ⊙은 원소별 곱샘을 뜻하고 r은 ‘masking’ 벡터로 각 값이 확률 p의 Bernoulli분포를 따르기 때문에 0 혹은 1의 값을 가짐. 
test과정에서는 dropout을 하지 않고 w 대신에 dropout 확률 p를 곱해서 사용. w^=pw<br>
<br>

**13.**  l2 정규화의 경우 가중치 벡터의 l2 norm이 s보다 클 경우 s값을 가지도록 조정.

![img](https://github.com/cym-2/cym-2.github.io/blob/main/assets/img/posts/ap1/10.PNG?raw=true)

<br>
<br>

## Experimental Setup
<br>

**14.** 이 모델에서 사용한 hyperparameter 및 함수.

- ReLU 함수
- 100 feature map으로 filter의 크기는 3,4,5로 지정
- Dropout 확률 : 0.5
- l2 값 : 3
- 미니 배치 크기 : 50
- early stopping이 추가적으로 사용됨.

**15.** 단어 벡터의 경우는 앞에서 나왔듯이 google news를 활용해 word2vec으로 사전 학습된 임베딩 벡터를 사용했으며 
만약 vocabulary에 들어가 있지 않은 단어의 경우에는 임의의 값으로 초기화해서 사용.<br>
<br>

**16.** Model Variation

- CNN-rand : baseline값으로 사용하기 위해 사용. 모든 단어 벡터를 임의의 값으로 초기화해서 사용.
- CNN-static : 앞서 말한 사전 학습된 word2vec 단어 벡터를 사용한 모델.
- CNN-non-static : 위의 모델과 같이 학습된 벡터를 사용했지만 각 task에서 벡터값은 update됨.
- CNN-multichannel : architecture 소개 부분에서 나왔듯이 input값을 1-channel로 한 것이 아니라, 2-channel인 모델. 
둘 다 word2vec으로 학습한 단어 벡터인데 하나는 static하게 값이 그대로이고 나머지 하나는 학습 중간 계속 update됨. 
즉 위의 static과 non-static을 섞어서 사용한 것과 같음.

<br>
<br>

## Results

![img](https://github.com/cym-2/cym-2.github.io/blob/main/assets/img/posts/ap1/11.jpg?raw=true)
