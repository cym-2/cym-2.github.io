I"ß><h1 id="backpropagation">Backpropagation</h1>
<p><br />
<br /></p>

<p><strong>0084.</strong> ìˆ˜ì¹˜ ë¯¸ë¶„ì€ ë‹¨ìˆœí•˜ê³  êµ¬í˜„í•˜ê¸° ì‰½ì§€ë§Œ ê³„ì‚° ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼-&gt; ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚°í•˜ëŠ” â€˜ì˜¤ì°¨ì—­ì „íŒŒë²•(backpropagation)â€™
<br /></p>

<hr />

<p><br /></p>

<p><strong>0085.</strong> ì—°ì‡„ë²•ì¹™: í•©ì„± í•¨ìˆ˜ì˜ ë¯¸ë¶„ì— ëŒ€í•œ ì„±ì§ˆ. í•©ì„± í•¨ìˆ˜ì˜ ë¯¸ë¶„ì€ í•©ì„± í•¨ìˆ˜ë¥¼ êµ¬ì„±í•˜ëŠ” ê° í•¨ìˆ˜ì˜ ë¯¸ë¶„ì˜ ê³±ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŒ.<br /></p>

<p><img src="http://www.sciweavers.org/upload/Tex2Img_1610791277/render.png" alt="img" /></p>

<p><img src="http://www.sciweavers.org/upload/Tex2Img_1610791336/render.png" alt="img" /></p>

<p><br /></p>

<p><img src="https://media.vlpt.us/post-images/dscwinterstudy/6b217c20-41a4-11ea-b40d-6705eaadcebd/e-5.2.png" alt="img" /></p>

<p><img src="https://media.vlpt.us/post-images/dscwinterstudy/6ed116a0-41a4-11ea-b40d-6705eaadcebd/e-5.3.png" alt="img" /></p>

<p><img src="https://media.vlpt.us/post-images/dscwinterstudy/71a87990-41a4-11ea-b40d-6705eaadcebd/e-5.4.png" alt="img" /></p>

<p><br /></p>

<p><img src="https://media.vlpt.us/post-images/dscwinterstudy/e47fd900-41a3-11ea-b40d-6705eaadcebd/fig-5-7.png" alt="img" /></p>

<p><img src="https://media.vlpt.us/post-images/dscwinterstudy/e10223f0-41a3-11ea-9e70-43b4cf1f0bf4/fig-5-8.png" alt="img" /></p>

<p><br /></p>

<hr />

<p><br /></p>

<p><strong>0086.</strong> ë§ì…ˆ ë…¸ë“œì˜ ì—­ì „íŒŒ. ë§ì…ˆ ë…¸ë“œì˜ ì—­ì „íŒŒëŠ” ì…ë ¥ ê°’ì„ ê·¸ëŒ€ë¡œ í˜ë ¤ë³´ëƒ„.</p>

<p><img src="https://media.vlpt.us/post-images/dscwinterstudy/55494a90-41a4-11ea-b40d-6705eaadcebd/fig-5-9.png" alt="img" /></p>

<p><br /></p>

<p><strong>0087.</strong> ê³°ì…ˆ ë…¸ë“œì˜ ì—­ì „íŒŒ. ìˆœì „íŒŒ ë•Œì˜ ì…ë ¥ ì‹ í˜¸ë“¤ì„ â€˜ì„œë¡œ ë°”ê¾¼ ê°’â€™ì„ ê³±í•˜ì—¬ í•˜ë¥˜ë¡œ ë³´ëƒ„. ê·¸ë˜ì„œ ê³±ì…ˆ ë…¸ë“œë¥¼ êµ¬í˜„í•  ë•ŒëŠ” ìˆœì „íŒŒì˜ ì…ë ¥ ì‹ í˜¸ë¥¼ ë³€ìˆ˜ì— ì €ì¥í•´ë‘ .<br /></p>

<p><img src="https://media.vlpt.us/post-images/dscwinterstudy/1be4a500-41a5-11ea-8248-4760a63b1878/fig-5-12.png" alt="img" /></p>

<p><br /></p>

<hr />

<p><br /></p>

<p><strong>0088.</strong> í™œì„±í™” í•¨ìˆ˜ ReLU ê³„ì¸µ êµ¬í˜„.<br /></p>

<p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbDxdMs%2FbtqAJhUyVEu%2F6xCtrkc6NUNH7J6cHH98m0%2Fimg.png" alt="img" /></p>

<p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbURUsW%2FbtqAK40NRNW%2FrlUqsXRGG5dkGFJViUVvQK%2Fimg.png" alt="img" /></p>

<p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FPgp1D%2FbtqAKhl4f1d%2FCU03m5gaYX0TsZyKxYcFAK%2Fimg.png" alt="img" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre></td> --><td class="rouge-code"><pre>class Relu:
  def __init__(self):
    self.mask = None # True/Falsleë¡œ êµ¬ì„±ëœ ë„˜íŒŒì´ ë°°ì—´ë¡œ, ìˆœì „íŒŒì˜ ì…ë ¥ì¸ xì˜ ì›ì†Œ ê°’ì´ 0ì´í•˜ì¸ ì¸ë±ìŠ¤ëŠ” True, ê·¸ ì™¸(0ë³´ë‹¤ í° ì›ì†Œ)ëŠ” Falseë¡œ ìœ ì§€.
    
  def forward(self, x):
    self.mask = (x &lt;= 0)
    out = x.copy()
    out[self.mask] = 0
    
    return out
    
  def backward(self, dout):
    dout[self.mask] = 0 # ì—­ì „íŒŒ ë•ŒëŠ” ìˆœì „íŒŒ ë•Œ ë§Œë“¤ì–´ë‘” maskë¥¼ ì¨ì„œ maskì˜ ì›ì†Œê°€ Trueì¸ ê³³ì—ëŠ” ìƒë¥˜ì—ì„œ ì „íŒŒëœ doutì„ 0ìœ¼ë¡œ ì„¤ì •.
    dx = dout
    
    return dx
</pre></td></tr></tbody></table></code></pre></div></div>

<p><br /></p>

<p><strong>0089.</strong> í™œì„±í™” í•¨ìˆ˜ Sigmoid ê³„ì¸µ êµ¬í˜„.<br /></p>

<p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbuXy9x%2FbtqALQgKeCz%2FREaaWJwaxHwamvvo9rBND0%2Fimg.png" alt="img" /></p>

<p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fk110u%2FbtqAJfWQykv%2FBEe2hqIiEYfTfWkuJg1sj0%2Fimg.png" alt="img" /></p>

<p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FPgE76%2FbtqALQHPMoc%2FzeIC4aiGVOVpdC28dX9ZFK%2Fimg.png" alt="img" /></p>

<ul>
  <li>â€™/â€™ë…¸ë“œì—ì„œ y = 1/x ë¥¼ ë¯¸ë¶„í•˜ë©´, -1/(x^2) = -y^2ì´ ë¨.<br /></li>
  <li>â€™+â€™ë…¸ë“œëŠ” ìƒë¥˜ì˜ ê°’ì„ ì—¬ê³¼ ì—†ì´ í•˜ë¥˜ë¡œ ë³´ë‚´ëŠ” ê²ƒì´ ì „ë¶€.<br /></li>
  <li>â€˜expâ€™ë…¸ë“œëŠ” y = exp(x)ì—°ì‚°ì„ ìˆ˜í–‰.<br /></li>
  <li>â€˜xâ€™ë…¸ë“œëŠ” ìˆœì „íŒŒ ë•Œì˜ ê°’ì„ ì„œë¡œ ë°”ê¿” ê³±í•¨.<br /></li>
</ul>

<p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FS4aIY%2FbtqAK4Gy0by%2FqiSZM4buvGUgxbFCurlO40%2Fimg.png" alt="img" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td> --><td class="rouge-code"><pre>class Sigmoid:
  def __init__(self):
    self.out = None
    
  def forward(self, x):
    out = 1 / (1 + np.exp(-x))
    self.out = out # ìˆœì „íŒŒì˜ ì¶œë ¥ì„ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ outì— ë³´ê´€í–ˆë‹¤ê°€, ì—­ì „íŒŒ ê³„ì‚° ë•Œ ê·¸ ê°’ì„ ì‚¬ìš©
    
    return out
    
  def backward(self, dout):
    dx = dout * (1.0 - self.out) * self.out
    
    return dx
</pre></td></tr></tbody></table></code></pre></div></div>

<p><br /></p>

<p><strong>0090.</strong> ë°°ì¹˜ìš© Affine(ì‹ ê²½ë§ì˜ ìˆœì „íŒŒ ë•Œ ìˆ˜í–‰í•˜ëŠ” í–‰ë ¬ì˜ ê³±) ê³„ì¸µ<br /></p>

<p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbg7kNw%2FbtqAK4tisBq%2F8uKeKfZdqqU8KKaLCEXF4k%2Fimg.png" alt="img" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre></td> --><td class="rouge-code"><pre>class Affine:
  def __init__(self, W, b):
    self.W = W
    self.b = b
    self.x = None
    self.dW = None
    self.db = None
  
  def forward(self, x):
    self.x = x
    out = np.dot(x, self.W) + self.b
    
    return out
    
  def backward(self, dout):
    dx = np.dot(dout, self.W.T)
    self.dW = np.dot(self.x.T, dout)
    self.db = np.sum(dout, axis=0)
    
    return dx
</pre></td></tr></tbody></table></code></pre></div></div>

<p><br /></p>

<p><strong>0091.</strong> Softmax-with-Loss ê³„ì¸µ(ì†ì‹¤ í•¨ìˆ˜ì¸ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ë„ í¬í•¨)<br /></p>

<p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FzGPE4%2FbtqALQH9zso%2FbpLtNaGnlk61XWurzui000%2Fimg.png" alt="img" /></p>

<p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbzX4Z3%2FbtqALt0HJ6j%2FyHOuB9073HnOwhgxlDExkK%2Fimg.png" alt="img" /></p>

<ul>
  <li>Softmax ê³„ì¸µì€ ì…ë ¥ (a1, a2, a3)ë¥¼ ì •ê·œí™”í•˜ì—¬ (y1, y2, y3)ë¥¼ ì¶œë ¥.</li>
  <li>Cross Entropy Error ê³„ì¸µì€ Softmaxì˜ ì¶œë ¥ (y1, y2, y3)ì™€ ì •ë‹µ ë ˆì´ë¸” (t1, t2, t3)ë¥¼ ë°›ê³ , ì´ ë°ì´í„°ë“¤ë¡œë¶€í„° ì†ì‹¤ Lì„ ì¶œë ¥.</li>
  <li>(y1, y2, y3)ëŠ” Softmax ê³„ì¸µì˜ ì¶œë ¥ì´ê³  (t1, t2, t3)ëŠ” ì •ë‹µ ë ˆì´ë¸”ì´ë¯€ë¡œ (y1-t1, y2-t2, y3-t3)ëŠ” Softmax ê³„ì¸µì˜ ì¶œë ¥ê³¼ ì •ë‹µ ë ˆì´ë¸”ì˜ ì°¨ë¶„.</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre></td> --><td class="rouge-code"><pre>class SoftmaxWithLoss:
  def __init__(self):
    self.loss = None # ì†ì‹¤
    self.y = None # softmaxì˜ ì¶œë ¥
    self.t = None # ì •ë‹µ ë ˆì´ë¸”(ì›í•«ë²¡í„°)
    
  def forward(self, x, t):
    self.t = t
    self.y = softmax(x)
    self.loss = cross_entropy_error(self.y, self.t)
    return self.loss
    
  def backward(self, dout=1):
    batch_size = self.t.shape[0]
    dx = (self.y - self.t) / batch_size
    
    return dx
</pre></td></tr></tbody></table></code></pre></div></div>

<p><br /></p>

<hr />

<p><br /></p>

<p><strong>0092.</strong> ì‹ ê²½ë§ í•™ìŠµì˜ ì „ì²´ ê·¸ë¦¼.<br /></p>

<ul>
  <li>[ì „ì²´]<br /></li>
  <li>ì‹ ê²½ë§ì—ëŠ” ì ì‘ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì´ ìˆê³ , ì´ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ í›ˆë ¨ ë°ì´í„°ì— ì ì‘í•˜ë„ë¡ ì¡°ì •í•˜ëŠ” ê³¼ì •ì„ â€˜í•™ìŠµâ€™ì´ë¼ í•¨.<br />
    <ul>
      <li>[1ë‹¨ê³„: ë¯¸ë‹ˆë°°ì¹˜] í›ˆë ¨ ë°ì´í„° ì¤‘ ì¼ë¶€ë¥¼ ë¬´ì‘ìœ„ë¡œ ê°€ì ¸ì˜´. ì´ë ‡ê²Œ ì„ ë³„í•œ ë°ì´í„°ë¥¼ ë¯¸ë‹ˆë°°ì¹˜ë¼ í•˜ë©°, ê·¸ ë¯¸ë‹ˆë°°ì¹˜ì˜ ì†ì‹¤ í•¨ìˆ˜ ê°’ì„ ì¤„ì´ëŠ” ê²ƒì´ ëª©í‘œ.<br /></li>
      <li>[2ë‹¨ê³„: ê¸°ìš¸ê¸° ì‚°ì¶œ(ìˆ˜ì¹˜ ë¯¸ë¶„, ì˜¤ì°¨ì—­ì „íŒŒë²•)] ë¯¸ë‹ˆë°°ì¹˜ì˜ ì†ì‹¤ í•¨ìˆ˜ ê°’ì„ ì¤„ì´ê¸° ìœ„í•´ ê° ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•¨. ê¸°ìš¸ê¸°ëŠ” ì†ì‹¤ í•¨ìˆ˜ì˜ ê°’ì„ ê°€ì¥ ì‘ê²Œ í•˜ëŠ” ë°©í–¥ì„ ì œì‹œ.<br /></li>
      <li>[3ë‹¨ê³„: ë§¤ê°œë³€ìˆ˜ ê°±ì‹ ] ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê¸°ìš¸ê¸° ë°©í–¥ìœ¼ë¡œ ì•„ì£¼ ì¡°ê¸ˆ ê°±ì‹ .<br /></li>
      <li>[4ë‹¨ê³„: ë°˜ë³µ] 1 ~ 3ë‹¨ê³„ ë°˜ë³µ<br /></li>
    </ul>
  </li>
</ul>

<p><br /></p>

<p><strong>0093.</strong> ì˜¤ì°¨ì—­ì „íŒŒë²•ì„ ì ìš©í•œ ì‹ ê²½ë§ êµ¬í˜„.<br /></p>

<p><em>TwoLayerNet í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜</em><br /></p>

<table>
  <thead>
    <tr>
      <th>ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜</th>
      <th>ì„¤ëª…</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>params</td>
      <td>ë”•ì…”ë„ˆë¦¬ ë³€ìˆ˜ë¡œ, ì‹ ê²½ë§ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ë³´ê´€.<br />params[â€˜W1â€™]ì€ 1ë²ˆì§¸ ì¸µì˜ ê°€ì¤‘ì¹˜, params[â€˜b1â€™]ì€ 1ë²ˆì§¸ ì¸µì˜ í¸í–¥</td>
    </tr>
    <tr>
      <td>layers</td>
      <td>ìˆœì„œê°€ ìˆëŠ” ë”•ì…”ë„ˆë¦¬ ë³€ìˆ˜ë¡œ, ì‹ ê²½ë§ì˜ ê³„ì¸µì„ ë³´ê´€.<br />layers[â€˜Affine1â€™], layers[â€˜Relu1â€™], layers[â€˜Affine2â€™]ì™€ ê°™ì´ ê° ê³„ì¸µì„ ìˆœì„œëŒ€ë¡œ ìœ ì§€</td>
    </tr>
    <tr>
      <td>lastLayer</td>
      <td>ì‹ ê²½ë§ì˜ ë§ˆì§€ë§‰ ê³„ì¸µ.<br />ì´ ì˜ˆì—ì„œëŠ” SoftmaxWithLoss ê³„ì¸µ.</td>
    </tr>
  </tbody>
</table>

<p><em>TwoLayerNet í´ë˜ìŠ¤ì˜ ë©”ì„œë“œ</em><br />
|ë©”ì„œë“œ|ì„¤ëª…|
|â€”â€”|â€”-|
|<strong>init</strong>(self, input_size, hidden_size, output_size, weight_init_std| ì´ˆê¸°í™”ë¥¼ ìˆ˜í–‰.<br />ì¸ìˆ˜ëŠ” ì•ì—ì„œë¶€í„° ì…ë ¥ì¸µ ë‰´ëŸ° ìˆ˜, ì€ë‹‰ì¸µ ë‰´ëŸ° ìˆ˜, ì¶œë ¥ì¸µ ë‰´ëŸ° ìˆ˜, ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ì‹œ ì •ê·œë¶„í¬ì˜ ìŠ¤ì¼€ì¼|
|predict(self, x)|ì˜ˆì¸¡(ì¶”ë¡ )ì„ ìˆ˜í–‰.<br />ì¸ìˆ˜ xëŠ” ì´ë¯¸ì§€ ë°ì´í„°|
|loss(self, x, t)|ì†ì‹¤ í•¨ìˆ˜ì˜ ê°’ì„ êµ¬í•¨.<br />ì¸ìˆ˜ xëŠ” ì´ë¯¸ì§€ ë°ì´í„°, tëŠ” ì •ë‹µ ë ˆì´ë¸”|
|accuracy(self, x, t)|ì •í™•ë„ë¥¼ êµ¬í•¨.|
|numerical_gradient(self, x, t)|ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ ìˆ˜ì¹˜ ë¯¸ë¶„ ë°©ì‹ìœ¼ë¡œ êµ¬í•¨.|
|gradient(self, x, t)|ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ ì˜¤ì°¨ì—°ì „íŒŒë²•ìœ¼ë¡œ êµ¬í•¨.|</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
</pre></td> --><td class="rouge-code"><pre>import sys, os
sys.path.append(os.pardir)
import numpy as np
from common.layers import *
from common.gradient import numerical_gradient
from collections import OrderedDict

class TwoLayerNet:
  
  def __init__(self, intput_size, hidden_size, output_size, weight_init_std=0.01):
    # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
    self.params = {}
    self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
    self.params['b1'] = np.zeros(hidden_size)
    self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
    self.params['b2'] = np.zeros(output_size)
    
    # ê³„ì¸µ ìƒì„±
    self.layers = OrderedDict()
    self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])
    self.layers['Relu1'] = Relu()
    self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])
    self.lastLayer = SoftmaxWithLoss()
    
  def predict(self, x):
    for layer in self.layers.values():
      x = layers.forward(x)
      
    return x
    
  # x: ì…ë ¥ ë°ì´í„°, t: ì •ë‹µ ë ˆì´ë¸”
  def loss(self, x, t):
    y = self.predict(x)
    
    return self.lastLayer.forward(y, t)
    
  def accuracy(self, x, t)
    y = self.predict(x)
    y = np.argmax(y, axis=1)
    if t.ndim != 1 : t = np.argmax(t, axis=1)
    
    accuracy = np.sum(y == t) / float(x.shape[0])
    return accuracy
    
  # x: ì…ë ¥ ë°ì´í„°, t: ì •ë‹µ ë ˆì´ë¸”
  def numerical_gradient(self, x, t):
    loss_W = lambda W: self.loss(x, t)
    
    grads = {}
    grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
    grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
    grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
    grads['b2'] = nemerical_gradient(loss_W, self.params['b2'])
    return grads
    
  def gradient(self, x, t):
    # ìˆœì „íŒŒ
    self.loss(x, t)
    
    # ì—­ì „íŒŒ
    dout = 1
    dout = self.lastLayer.backward(dout)
    
    layers = list(self.layers.values())
    layers.reverse()
    for layer in layers:
      dout = layer.backward(dout)
      
    # ê²°ê³¼ ì €ì¥
    grads = {}
    grads['W1'] = self.layers['Affine1'].dW
    grads['b1'] = self.layers['Affine1'].db
    grads['W2'] = self.layers['Affine2'].dW
    grads['b2'] = self.layers['Affine2'].db
    
    return grads
</pre></td></tr></tbody></table></code></pre></div></div>

<p><br /></p>

<p><strong>0094.</strong> ì˜¤ì°¨ì—­ì „íŒŒë²•ì„ ì‚¬ìš©í•œ í•™ìŠµ êµ¬í˜„í•˜ê¸°.<br /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
</pre></td> --><td class="rouge-code"><pre>import sys, os
sys.path.append(os.pardir)
import numpy as np
from dataset.mnist import load_mnis
from two_layer_net import TwoLayerNet

# ë°ì´í„° ì½ê¸°
(x_train, t_train), (x_test, t_test) = load_mnist(normaliz=True, ont_hot_label=True)
network = TwoLayerNet(intput_size=784, hidden_size=50, output_size=10)

iters_num = 10000
train_size = x_train.shape[0]
batch_size = 100
learning_rate = 0.1

train_loss_list = []
train_acc_list = []
test_acc_list = []

iter_per_epoch = max(train_size / batch_size, 1)

for i in range(iter_num):
  batch_mask = np.random.choice(train_size, batch_size)
  x_batch = x_train[batch_mask]
  t_batch = t_train[batch_mask]
  
  # ì˜¤ì°¨ì—­ì „íŒŒë²•ìœ¼ë¡œ ê¸°ìš¸ê¸° êµ¬í•¨.
  grad = netword.gradient(x_batch, t_batch)
  
  # ê°±ì‹ 
  for key in ('W1', 'b1', 'W2', 'b2'):
    network.params[key] -= learning_rate * grad[key]
    
  loss = network.loss(x_batch, t_batch)
  train_loss_list.append(loss)
  
  if i % iter_per_epock == 0:
    train_acc = network.accuracy(x_train, t_train)
    test_acc = network.accuracy(x_test, t_test)
    train_acc_list.append(train_acc)
    test_acc_list.append(test_acc)
    print(train_acc, test_acc)
</pre></td></tr></tbody></table></code></pre></div></div>

<p><br /></p>

<p><strong>0095.</strong> ê¸°ìš¸ê¸° í™•ì¸</p>

<ul>
  <li>ìˆ˜ì¹˜ ë¯¸ë¶„ì€ ì˜¤ì°¨ì—­ì „íŒŒë²•ì„ ì •í™•íˆ êµ¬í˜„í–ˆëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ í•„ìš”.<br /></li>
  <li>ìˆ˜ì¹˜ ë¯¸ë¶„ì€ êµ¬í˜„í•˜ê¸° ì‰¬ì›€.<br /></li>
  <li>ìˆ˜ì¹˜ ë¯¸ë¶„ì˜ êµ¬í˜„ì—ëŠ” ë²„ê·¸ê°€ ìˆ¨ì–´ìˆê¸° ì–´ë ¤ìš´ ë°˜ë©´, ì˜¤ì°¨ì—­ì „íŒŒë²•ì€ êµ¬í˜„í•˜ê¸° ë³µì¡í•´ì„œ ì¢…ì¢… ì‹¤ìˆ˜ë¥¼ í•˜ê³¤ í•¨.<br /></li>
  <li>ìˆ˜ì¹˜ ë¯¸ë¶„ì˜ ê²°ê³¼ì™€ ì˜¤ì°¨ì—­ì „íŒŒë²•ì˜ ê²°ê³¼ë¥¼ ë¹„êµí•˜ì—¬ ì˜¤ì°¨ì—­ì „íŒŒë²•ì„ ì œëŒ€ë¡œ êµ¬í˜„í–ˆëŠ”ì§€ ê²€ì¦í•˜ê³¤ í•¨.<br /></li>
</ul>
:ET